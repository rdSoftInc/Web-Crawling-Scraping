# Web-Crawling-Scraping

A web crawler & scraper, to crawl & scrap websites for relevant sites & data...

# Required Packages

run "npm install" cmd in the terminal

# Command to run 

node ./src/index.js

# Tasks - âœ… - Completed | ðŸŸ§ - In Progress | ðŸ”² - Not Started 

## Task 1 : Build a basic node js server & implement a web crawler function to find links & sub-links for a given url & store it in a json file âœ… - Rolstan

### Completed : Run the script with cmd as "node src/index.js" & wait for it to finish, once execution completes, find the urls stored in prerequisites/horizon.json file for further analysis.

## Task 2 : Analyze the urls stored in prerequisites/horizon.json file & create pre-data fetch filter for processing required information. âœ… - Robin & Giyosiddin 

### Completed : Successfully identified the non essential urls & the pre-data-fetch filter parameters.

## Task 3 : Create data model to support the required data for the base urls. âœ… - Robin & Rolstan

### Completed : Created a js data model to capture enough data for initial stage. 

## Task 4-A : Build a web scraper to scrape data from the urls obtained from the web crawler. ðŸŸ§ - Robin

## Task 4-B : Create a function to connect to a firebase database, data validation & storing of data in the firebase. ðŸŸ§ - Giyosiddin

## Task 4-C : Edit index.js file / Create a function to make sure only the new urls are stored in the json object & are sent to be stored in the firebase. ðŸŸ§ - Rolstan

## Task 5 : Crawl all the urls provided in urls.json file as a base url, so the above tasks can be performed for each base url. ðŸ”² - Yet to be assigned
